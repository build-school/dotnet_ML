#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"},{"aliases":["frontend"],"name":"vscode"}]}}

#!fsharp

//#r "nuget: torchsharp-cpu" //for cpu
//#r "nuget: libtorch-cuda-11.3-win-x64"
#r "nuget: torchsharp-cuda-windows" //for gpu
#r "nuget: TorchSharp"
#r "nuget: TfCheckpoint"   
#r "nuget: FsBERTTokenizer"
#r "nuget: FSharp.Data"
open TfCheckpoint
open TorchSharp

let device = if torch.cuda_is_available() then torch.CUDA else torch.CPU
printfn $"torch devices is %A{device}"

#!fsharp

open TfCheckpoint
open TorchSharp
let bertCheckpointFolder = @".\Variables"
let tensors = CheckpointReader.readCheckpoint bertCheckpointFolder |> Seq.toArray
//show first tensor
printfn "%A" tensors.[0]

#!fsharp

tensors (* |> Array.skip 20 *) |> Array.map (fun (n,st) -> {|Dims=st.Shape; Name=n|})

#!fsharp

//tensor dims - these values should match the relevant dimensions of the corresponding tensors in the checkpoint
let HIDDEN      = 128L
let VOCAB_SIZE  = 30522L    // see vocab.txt file included in the BERT download
let TYPE_SIZE   = 2L         // bert needs 'type' of token
let MAX_POS_EMB = 512L

//other parameters
let EPS_LAYER_NORM      = 1e-12
let HIDDEN_DROPOUT_PROB = 0.1
let N_HEADS             = 2L
let ATTN_DROPOUT_PROB   = 0.1
let ENCODER_LAYERS      = 2L
let ENCODER_ACTIVATION  = torch.nn.Activations.GELU

#!fsharp

//Note: The module and variable names used here match the tensor name 'paths' as delimted by '/' for TF (see above), 
//for easier matching.
type BertEmbedding() as this = 
    inherit torch.nn.Module("embeddings")
    
    let word_embeddings         = torch.nn.Embedding(VOCAB_SIZE,HIDDEN,padding_idx=0L)
    let position_embeddings     = torch.nn.Embedding(MAX_POS_EMB,HIDDEN)
    let token_type_embeddings   = torch.nn.Embedding(TYPE_SIZE,HIDDEN)
    let LayerNorm               = torch.nn.LayerNorm([|HIDDEN|],EPS_LAYER_NORM)
    let dropout                 = torch.nn.Dropout(HIDDEN_DROPOUT_PROB)

    do 
        this.RegisterComponents()

    member this.forward(input_ids:torch.Tensor, token_type_ids:torch.Tensor, position_ids:torch.Tensor) =   
    
        let embeddings =      
            (input_ids       --> word_embeddings)        +
            (token_type_ids  --> token_type_embeddings)  +
            (position_ids    --> position_embeddings)

        embeddings --> LayerNorm --> dropout             // the --> operator works for simple 'forward' invocations

#!fsharp

type BertPooler() as this = 
    inherit torch.nn.Module<torch.Tensor,torch.Tensor>("pooler")

    let dense = torch.nn.Linear(HIDDEN,HIDDEN)
    let activation = torch.nn.Tanh()

    let ``:`` = torch.TensorIndex.Colon
    let first = torch.TensorIndex.Single(0L)

    do
        this.RegisterComponents()

    override _.forward (hidden_states) =
        let first_token_tensor = hidden_states.index(``:``, first) //take first token of the sequence as the pooled value
        first_token_tensor --> dense --> activation

#!fsharp

type BertModel() as this =
    inherit torch.nn.Module("bert")

    let embeddings = new BertEmbedding()
    let pooler = new BertPooler()

    let encoderLayer = torch.nn.TransformerEncoderLayer(HIDDEN, N_HEADS, MAX_POS_EMB, ATTN_DROPOUT_PROB, activation=ENCODER_ACTIVATION)
    let encoder = torch.nn.TransformerEncoder(encoderLayer, ENCODER_LAYERS)

    do
        this.RegisterComponents()
    
    member this.forward(input_ids:torch.Tensor, token_type_ids:torch.Tensor, position_ids:torch.Tensor,?mask:torch.Tensor) =
        let src = embeddings.forward(input_ids, token_type_ids, position_ids)
        let srcBatchDim2nd = src.permute(1L,0L,2L) //PyTorch transformer requires input as such. See the Transformer docs
        let encoded = match mask with None -> encoder.forward(srcBatchDim2nd, null, null) | Some mask -> encoder.forward(srcBatchDim2nd,mask, null)
        let encodedBatchFst = encoded.permute(1L,0L,2L)
        encodedBatchFst --> pooler

#!fsharp

let testBert = new BertModel()
//bert.named_modules() 
testBert.named_parameters() |> Seq.map (fun struct(n,x) -> n,x.shape) |> Seq.iter (printfn "%A")

#!fsharp

open System
open System.IO

#!fsharp

module Tensor = 
    //Note: ensure 't matches tensor datatype otherwise ToArray might crash the app (i.e. exception cannot be caught)
    let private _getData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t) and 't:unmanaged > (t:torch.Tensor) =
        let s = t.data<'t>()
        s.ToArray()

    let getData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t) and 't:unmanaged>  (t:torch.Tensor) =
        if t.device_type <> DeviceType.CPU then 
            //use t1 = t.clone()
            use t2 = t.cpu()
            _getData<'t> t2
        else 
            _getData<'t> t
  
    let setData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t) and 't:unmanaged> (t:torch.Tensor) (data:'t[]) =
        if t.device_type = DeviceType.CPU |> not then failwith "tensor has to be on cpu for setData"        
        let s = t.data<'t>()
        s.CopyFrom(data,0,0L)

#!fsharp

    let private _getData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t) and 't:unmanaged > (t:torch.Tensor) =
        let s = t.data<'t>()
        s.ToArray()
