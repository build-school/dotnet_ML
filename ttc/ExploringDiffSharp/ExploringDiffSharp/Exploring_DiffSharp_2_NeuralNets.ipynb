{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring DiffSharp: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Microsoft.DotNet.Interactive.InstallPackagesMessage\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: DiffSharp-lite, 1.0.1\"\n",
    "\n",
    "Formatter.SetPreferredMimeTypesFor(typeof<obj>, \"text/plain\")\n",
    "Formatter.Register(fun x writer -> fprintfn writer \"%120A\" x )\n",
    "\n",
    "open DiffSharp\n",
    "open DiffSharp.Util // required for print and other stuff\n",
    "open DiffSharp.Model\n",
    "open System.Diagnostics   // not required\n",
    "open System               // not required\n",
    "\n",
    "dsharp.config(dtype=Dtype.Float32, device=Device.CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Layers\n",
    "The below cell defines a Linear Layer where the bias can be turned off by setting **bias** to *false*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "type LinearB(inFeatures, outFeatures, ?bias: bool) =\n",
    "    inherit Model()\n",
    "    let bias = defaultArg bias true\n",
    "\n",
    "    let w = Parameter(Weight.kaiming(inFeatures, outFeatures))  // defining a parameter and its initialization function\n",
    "    let k = 1./sqrt (float outFeatures)\n",
    "    let b = Parameter(Weight.uniform([|outFeatures|], k))\n",
    "    do base.addParameter([w;b],[\"Linear-weight\";\"Linear-bias\"])\n",
    "\n",
    "    override _.ToString() = sprintf \"Linear(%A, %A)\" inFeatures outFeatures\n",
    "    \n",
    "    override _.forward(value) =\n",
    "        if bias then dsharp.matmul(value, w.value) + b.value else dsharp.matmul(value, w.value) + 0. * b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an initialization function\n",
    "Next we define a Linear Layer that allows both, biases to be turned of as well as a custom initialization function\n",
    "and then we create a new initialization function.\n",
    "\n",
    "Most predefined Layers do not have the option to specify an initialization function, but with a few changes they can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "type LinearBI(inFeatures, outFeatures, ?bias: bool, ?initFun) =\n",
    "    inherit Model()\n",
    "    let bias = defaultArg bias true\n",
    "    let initFun = defaultArg initFun Weight.kaiming\n",
    "\n",
    "    let w = Parameter(initFun(inFeatures, outFeatures))  // defining a parameter and its initialization function\n",
    "    let k = 1./sqrt (float outFeatures)\n",
    "    let b = Parameter(Weight.uniform([|outFeatures|], k))\n",
    "    do base.addParameter([w;b],[\"Linear-weight\";\"Linear-bias\"])\n",
    "\n",
    "    override _.ToString() = sprintf \"Linear(%A, %A)\" inFeatures outFeatures\n",
    "    \n",
    "    override _.forward(value) =\n",
    "        if bias then dsharp.matmul(value, w.value) + b.value else dsharp.matmul(value, w.value) + 0. * b.value\n",
    "\n",
    "\n",
    "type InitLayer =                                            // this initialization function makes no sense it is just intended as an example\n",
    "    static member initfunx(fanIn, fanOut, ?a: float) = \n",
    "        let a = defaultArg a (3.141528)\n",
    "        dsharp.randint(-3, 23, [fanIn; fanOut]) * a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Custom init function:\n",
      "tensor([[-14.2258],\n",
      "        [-29.3985],\n",
      "        [-44.5711]])\n",
      "------------------------------------------------------\"\n",
      "\"default init function:\n",
      "tensor([[-0.6649],\n",
      "        [-0.6827],\n",
      "        [-0.7005]])\n",
      "------------------------------------------------------\"\n",
      "\"Custom init function wrapped by another function:\n",
      "tensor([[-14.2258],\n",
      "        [-29.3985],\n",
      "        [-44.5711]])\"\n"
     ]
    }
   ],
   "source": [
    "dsharp.seed(23)\n",
    "\n",
    "// creating two models with a custom init function\n",
    "let model_custom = \n",
    "    LinearBI(1, 10, false, InitLayer.initfunx)\n",
    "    --> dsharp.relu\n",
    "    --> Linear(10, 1)\n",
    "\n",
    "dsharp.seed(23)\n",
    "let model_default = \n",
    "    LinearBI(1, 10, false)\n",
    "    --> dsharp.relu\n",
    "    --> Linear(10, 1)\n",
    "\n",
    "\n",
    "\n",
    "let test_data = dsharp.tensor([1., 2., 3.])\n",
    "print $\"Custom init function:\\n{test_data.view([-1; 1]) --> model_custom}\\n------------------------------------------------------\"\n",
    "print $\"default init function:\\n{test_data.view([-1; 1]) --> model_default}\\n------------------------------------------------------\"\n",
    "\n",
    "\n",
    "// if the initialization function takes more arguments or different arguments \n",
    "// e.g. a shape like [|inFeatures, outFeatures|] creating a function like this works:\n",
    "let initfunx2(fanIn, fanOut) = \n",
    "    InitLayer.initfunx(fanIn, fanOut, 3.141528)   \n",
    "    // 3.141528 is the default value of the init function, therefore the result below will be identical to the first one\n",
    "    // change the value to obtain different results\n",
    "\n",
    "dsharp.seed(23)\n",
    "let model3 = \n",
    "    LinearBI(1, 10, false, initfunx2)\n",
    "    --> dsharp.relu\n",
    "    --> Linear(10, 1)\n",
    "\n",
    "print $\"Custom init function wrapped by another function:\\n{test_data.view([-1; 1]) --> model3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Datasets\n",
    "DiffSharp offers a few ways to handle the most common types of Datasets:\n",
    "- Images\n",
    "- Text\n",
    "- Tensors (e.g. time series)\n",
    "\n",
    "DiffSharp.Data contains the necessary utilities, TensorDataset, ImageDataset, TextDataset.  \n",
    "Furthermore there's a few well known Datasets available: CIFAR10, CIFAR100, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(256)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39993852\n",
      "\"Dataset(256)\"\n",
      "\"Dataset(256)\n",
      "\"\n",
      "256\n",
      "(tensor(0.9634), tensor(16.7286))\n",
      "DiffSharp.Data.TensorDataset\n",
      "Dataset(98)\n",
      "true\n",
      "DiffSharp.Data.DataLoader\n"
     ]
    }
   ],
   "source": [
    "open DiffSharp.Data\n",
    "\n",
    "let x_data = (dsharp.rand([256]) - 0.5) * 9\n",
    "\n",
    "// a polynomial of degree 4\n",
    "let poly(x:  Tensor) = \n",
    "    x * x * x * x + x * x * x - 11 * x * x - 5 * x + 30\n",
    "\n",
    "let y_data = poly(x_data)\n",
    "let epochs = 4000\n",
    "\n",
    "// TensorDataset takes the input data as a tensor and the labels/target as a tensor\n",
    "let ds = TensorDataset(x_data, y_data)\n",
    "\n",
    "ds.Display()\n",
    "print(ds.GetHashCode())\n",
    "print(ds.ToString())\n",
    "print(ds.ToDisplayString())\n",
    "print(ds.length)    // get the length of the dataset\n",
    "print(ds.Item(0))   // get specific items\n",
    "print(ds.GetType()) // get the type of the dataset e.g. TensorDataSet, ImageDataset ...\n",
    "print(ds.filter(fun x y -> (float x) > 1.))  \n",
    "// filters and returns the dataset where x in this case corresponds to the values from \n",
    "// x_data and y corresponds to the values from y_data\n",
    "print(ds.Equals(ds))\n",
    "print(ds.loader(2, true, false))  // returns a DataLoader, specified by the options below\n",
    "// available options:\n",
    "//   batchSize    : int  *\n",
    "//   shuffle      : option<bool>  *\n",
    "//   dropLast     : option<bool>  *\n",
    "//   device       : option<Device>  *   // typically the dataset is on the CPU\n",
    "//   dtype        : option<Dtype>  *\n",
    "//   backend      : option<Backend>  *\n",
    "//   targetDevice : option<Device>  *   // device where the actual training is done\n",
    "//   targetDtype  : option<Dtype>  *\n",
    "//   targetBackend: option<Backend> \n",
    "\n",
    "// here's the train loader that is used to train the NN in the next chapter\n",
    "let batchsize = 128\n",
    "let trainLoader = ds.loader(batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataLoader\n",
    "These are the most important functions related to the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.9634, -3.4970, -4.0286, ..., -3.2670,  3.9466,  3.9709]),\n",
      " tensor([ 16.7286,  19.7514,  69.6332, ...,   7.9779,  143.0090,  147.9452]))\n",
      "seq\n",
      "  [(0, tensor([ 0.9634, -3.4970, -4.0286, ..., -3.2670,  3.9466,  3.9709]),\n",
      "    tensor([ 16.7286,  19.7514,  69.6332, ...,   7.9779,  143.0090,  147.9452]));\n",
      "   (1, tensor([-0.3838,  4.0720,  3.5992, ..., -0.3668,  2.6469, -2.9336]),\n",
      "    tensor([ 30.2640,  169.7124,  83.9454, ...,  30.3228,   7.3297,  -1.1810]))]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(trainLoader.batch()) // returns a single batch of inputs and targets of the previously\n",
    "                           // selected size (can optionally be specified to a different value)\n",
    "\n",
    "print(trainLoader.epoch()) // trainLoader.epoch() returns a sequence of the batch number\n",
    "                           // the inputs and the targets of the batch\n",
    "                           \n",
    "print(trainLoader.length)  // returns the number of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training Neural Networks\n",
    "Several loss functions are included by default:\n",
    "- dsharp.nllLoss\n",
    "- dsharp.crossEntropyLoss\n",
    "- dsharp.mseLoss\n",
    "- dsharp.bceLoss\n",
    "\n",
    "And there also are quite a few [layers](https://diffsharp.github.io/reference/diffsharp-model.html) available.\n",
    "\n",
    "When it comes to optimizers there's only two:\n",
    "- SGD\n",
    "- Adam\n",
    "\n",
    "but it is possible to create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3825.5364):rev\n",
      "tensor(2993.2634):rev\n",
      "tensor(463.0147):rev\n",
      "tensor(347.4586):rev\n",
      "tensor(171.7870):rev\n",
      "tensor(143.2018):rev\n",
      "tensor(124.1993):rev\n",
      "tensor(109.6401):rev\n",
      "tensor(140.3423):rev\n",
      "tensor(103.1965):rev\n",
      "\"Epoch: 500\"\n",
      "tensor(131.7450):rev\n",
      "tensor(70.1396):rev\n",
      "tensor(101.1958):rev\n",
      "tensor(90.9501):rev\n",
      "tensor(103.9215):rev\n",
      "tensor(82.2672):rev\n",
      "tensor(90.5190):rev\n",
      "tensor(81.5198):rev\n",
      "tensor(79.3801):rev\n",
      "tensor(71.3603):rev\n",
      "\"Epoch: 1000\"\n",
      "tensor(73.0621):rev\n",
      "tensor(62.0746):rev\n",
      "tensor(72.1376):rev\n",
      "tensor(62.4026):rev\n",
      "tensor(71.6132):rev\n",
      "tensor(64.1625):rev\n",
      "tensor(71.9583):rev\n",
      "tensor(62.5870):rev\n",
      "tensor(75.8764):rev\n",
      "tensor(74.1071):rev\n",
      "\"Epoch: 1500\"\n",
      "tensor(70.1039):rev\n",
      "tensor(63.3726):rev\n",
      "tensor(71.0166):rev\n",
      "tensor(62.7544):rev\n",
      "tensor(67.1107):rev\n",
      "tensor(64.0922):rev\n",
      "tensor(67.3875):rev\n",
      "tensor(64.8878):rev\n",
      "tensor(68.4382):rev\n",
      "tensor(60.8917):rev\n",
      "\"Epoch: 2000\"\n",
      "tensor(67.3343):rev\n",
      "tensor(61.7994):rev\n",
      "tensor(66.0873):rev\n",
      "tensor(63.2342):rev\n",
      "tensor(67.7834):rev\n",
      "tensor(62.1462):rev\n",
      "tensor(65.5904):rev\n",
      "tensor(61.5742):rev\n",
      "tensor(65.4582):rev\n",
      "tensor(61.2163):rev\n",
      "\"Epoch: 2500\"\n",
      "tensor(66.2846):rev\n",
      "tensor(60.9182):rev\n",
      "tensor(64.4066):rev\n",
      "tensor(64.2650):rev\n",
      "tensor(66.3828):rev\n",
      "tensor(59.7336):rev\n",
      "tensor(68.6820):rev\n",
      "tensor(62.1310):rev\n",
      "tensor(67.3018):rev\n",
      "tensor(60.5949):rev\n",
      "\"Epoch: 3000\"\n",
      "tensor(63.2649):rev\n",
      "tensor(59.6299):rev\n",
      "tensor(62.8612):rev\n",
      "tensor(59.6163):rev\n",
      "tensor(62.4402):rev\n",
      "tensor(59.5508):rev\n",
      "tensor(62.5968):rev\n",
      "tensor(59.5138):rev\n",
      "tensor(62.6932):rev\n",
      "tensor(59.2040):rev\n",
      "\"Epoch: 3500\"\n",
      "tensor(62.6670):rev\n",
      "tensor(59.5042):rev\n",
      "tensor(62.6450):rev\n",
      "tensor(58.9312):rev\n",
      "tensor(62.3919):rev\n",
      "tensor(59.1356):rev\n",
      "tensor(61.9470):rev\n",
      "tensor(59.4042):rev\n",
      "tensor(61.9325):rev\n",
      "tensor(58.8850):rev\n",
      "\"Epoch: 4000\"\n"
     ]
    }
   ],
   "source": [
    "open DiffSharp.Optim\n",
    "open DiffSharp.Compose\n",
    "\n",
    "// defining a custom loss function \n",
    "let absolute_loss(input, target) = \n",
    "    dsharp.sum(abs(input - target))\n",
    "\n",
    "// models can be very conveniently composed like this:\n",
    "let model = \n",
    "    LinearB(1, 36)\n",
    "    --> dsharp.leakyRelu(0.1)\n",
    "    --> LinearB(36, 10)\n",
    "    --> dsharp.relu\n",
    "    --> LinearB(10, 1, false)\n",
    "\n",
    "// defining several optimizers to imitate a learn rate schedule\n",
    "let optimizer = Adam(model, dsharp.tensor(0.0075))\n",
    "let optimizer2 = Adam(model, dsharp.tensor(0.002))\n",
    "let optimizer3 = Adam(model, dsharp.tensor(0.0005))\n",
    "\n",
    "for epoch = 1 to epochs do\n",
    "    let batches = trainLoader.epoch()\n",
    "    for i, input, target in batches do\n",
    "        model.reverseDiff()\n",
    "\n",
    "        let output = input.view([-1; 1]) --> model\n",
    "\n",
    "        // mean squared error loss\n",
    "        //let msel = dsharp.mseLoss(output, target.unsqueeze(1))    \n",
    "        //msel.reverse()\n",
    "\n",
    "        // absolute loss\n",
    "        let abserr = absolute_loss(output, target.unsqueeze(1))\n",
    "        abserr.reverse()\n",
    "\n",
    "        if epoch % 100 = 0 then\n",
    "            print(abserr)           // prints the errror every 100 epochs\n",
    "\n",
    "        // since theres no learnrate scheduling, here is a naive way to\n",
    "        // implement it:\n",
    "        if epoch <= 1000 then\n",
    "            optimizer.step()\n",
    "        if epoch > 1000  && epoch <= 3000 then\n",
    "            optimizer2.step()\n",
    "        if epoch > 3000 then\n",
    "            optimizer3.step()\n",
    "\n",
    "    if epoch % 500 = 0 then\n",
    "        print $\"Epoch: {epoch}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples, for example a RNN, GAN, VAE, VAE-CNN and classifier can be found at the official DiffSharp Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "DiffSharp offers a few simple options to create plots. (open DiffSharp.Util)  \n",
    "You can create line plots and histograms and save them to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let x_order = dsharp.arange(6.1, 0, 0.1) - 3 \n",
    "let y_order = poly(x_order)\n",
    "\n",
    "let res = x_order.view([-1; 1]) --> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let plt = Pyplot(\"/home/martin/anaconda3/bin/python\")     // insert path to python binary here\n",
    "plt.plot(x_order, y_order, label=\"true fun\")              // actual function that was approximated\n",
    "plt.plot(x_order, res.squeeze(), label=\"approx fun\")      // approximation of the function\n",
    "plt.plot(x_order, y_order - res.squeeze(), label=\"err\")   // error line\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.tightLayout()\n",
    "plt.savefig(\"true_fun_vs_approx_fun_ipy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let plt = Pyplot(\"/home/martin/anaconda3/bin/python\")     \n",
    "plt.plot(x_order, y_order - res.squeeze(), label=\"err\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.tightLayout()\n",
    "plt.savefig(\"Error_ipy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line plot with only y data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let plt = Pyplot(\"/home/martin/anaconda3/bin/python\")     \n",
    "plt.plot(y_order, label=\"Some metric/s\") // if x isn't specified x starts at 0 and increases by 1\n",
    "plt.xlabel(\"s\")                          // up to the total number of points - 1 \n",
    "plt.ylabel(\"Some metric\")\n",
    "plt.legend()\n",
    "plt.savefig(\"line_example_plot_ipy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let plt = Pyplot(\"/home/martin/anaconda3/bin/python\")     \n",
    "plt.hist(x_order, y_order, bins=10, density=false, label=\"Some metric/s\") \n",
    "plt.xlabel(\"s\")\n",
    "plt.ylabel(\"Some metric\")\n",
    "plt.legend()\n",
    "plt.savefig(\"hist_example_plot_ipy\")\n",
    "\n",
    "// options for hist\n",
    "// x : Tensor\n",
    "// ?weights : Tensor\n",
    "// ?bins : int\n",
    "// ?density : bool\n",
    "// ?label : string "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "file_extension": ".cs",
   "mimetype": "text/x-csharp",
   "name": "C#",
   "pygments_lexer": "csharp",
   "version": "9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
